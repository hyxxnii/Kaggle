{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data 살펴보기"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/sejongai-challenge-pretest-1/train.csv')\ntest_df = pd.read_csv('/kaggle/input/sejongai-challenge-pretest-1/test_data.csv')\nsubmission = pd.read_csv('/kaggle/input/sejongai-challenge-pretest-1/submit_sample.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_df.head())\ndisplay(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop('Unnamed: 0', axis=1, inplace=True)\ntest_df.drop('Unnamed: 0', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['8'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_copy = train_df.copy()\n\nX_features = df_copy.iloc[:, :-1]\ny_target = df_copy.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n\nprint((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    \n    # ROC-AUC\n    roc_auc = roc_auc_score(y_test, pred_proba) # 각 클래스에 대한 확률\n    print('오차 행렬')\n    print(confusion)\n    \n    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, AUC:{3:.4f}'.format(accuracy, precision, recall, roc_auc))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\nrf_clf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\nxgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=5, min_child_weight=1.0, random_state=0)\nxgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred = xgb_clf.predict(X_test)\nxgb_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]\n\ndt_clf = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_train_eval(y_test, clf, clf_pred, clf_pred_proba):\n    print('## {} 예측 성능'.format(clf.__class__.__name__))\n    get_clf_eval(y_test, clf_pred, clf_pred_proba)\n    print('\\n')\n\nget_model_train_eval(y_test, lr_clf, lr_pred, lr_pred_proba)\nget_model_train_eval(y_test, rf_clf, rf_pred, rf_pred_proba)\nget_model_train_eval(y_test, xgb_clf, xgb_pred, xgb_pred_proba)\nget_model_train_eval(y_test, dt_clf, dt_pred, dt_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA & feature engineering"},{"metadata":{},"cell_type":"markdown","source":"### IQR을 이용한 이상치 제거"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(9,9))\ncorr = train_df.corr()\nsns.heatmap(corr, cmap='RdBu', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결정 레이블인 class와 음의 상관관계가 가장 높은 피처는 1번과 5번\n# 1번에 대해서만 이상치를 찾아 제거해보자\n\ndiabetes = train_df[train_df['8']==1]['1']\nquantile_25 = np.percentile(diabetes.values, 25)\nquantile_75 = np.percentile(diabetes.values, 75)\n\n# IQR을 구하고, IQR에 1.5를 곱해 최댓갑과 최솟값 지점 구함\niqr = quantile_75 - quantile_25\niqr_weight = iqr * 1.5\nlowest_val = quantile_25 - iqr_weight\nhighest_val = quantile_75 + iqr_weight\n\noutlier_index = diabetes[(diabetes < lowest_val) | (diabetes > highest_val)].index\nprint('이상치 데이터 인덱스: ', outlier_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.drop(outlier_index, axis=0, inplace=True)\ndf_copy.reset_index(drop=True, inplace=True) # 1번 칼럼 이상치 제거\n\nX_features = df_copy.iloc[:, :-1]\ny_target = df_copy.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n\nprint((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\n#rf_clf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\n#xgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=5, min_child_weight=1.0, random_state=0)\nxgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred = xgb_clf.predict(X_test)\nxgb_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]\n\n#dt_clf = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_train_eval(y_test, lr_clf, lr_pred, lr_pred_proba)\nget_model_train_eval(y_test, rf_clf, rf_pred, rf_pred_proba)\nget_model_train_eval(y_test, xgb_clf, xgb_pred, xgb_pred_proba)\nget_model_train_eval(y_test, dt_clf, dt_pred, dt_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RF 특성 중요도\nprint(\"{} 특성 중요도 : \\n{}\".format(rf_clf.__class__.__name__, rf_clf.feature_importances_))\n\n\n# 특성 중요도 시각화 하기\ndef plot_feature_importances_cancer(model):\n    n_features = X_features.shape[1]\n    plt.barh(range(n_features), rf_clf.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_features.columns)\n    plt.xlabel(\"feature importances\")\n    plt.ylabel(\"feature\")\n    plt.ylim(-1, n_features)\n    plt.title('Random Forest Feature Importance')\n\nplt.show()\n\nplot_feature_importances_cancer(rf_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB 특성 중요도\nprint(\"{} 특성 중요도 : \\n{}\".format(xgb_clf.__class__.__name__, xgb_clf.feature_importances_))\n\n\n# 특성 중요도 시각화 하기\ndef plot_feature_importances_cancer(model):\n    n_features = X_features.shape[1]\n    plt.barh(range(n_features), xgb_clf.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_features.columns)\n    plt.xlabel(\"feature importances\")\n    plt.ylabel(\"feature\")\n    plt.ylim(-1, n_features)\n    plt.title('XGB Feature Importance')\n\nplt.show()\n\nplot_feature_importances_cancer(xgb_clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature 분포 확인"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfor col in X_features.columns:\n    flg, ax = plt.subplots(1, 1, figsize=(8,5))\n    sns.distplot(df_copy[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_df = df_copy.drop('8', axis=1).copy()\nlog_df = np.log1p(log_df)\nlog_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features = log_df\ny_target = df_copy.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n\nprint((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\nrf_clf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\nxgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=5, min_child_weight=1.0, random_state=0)\nxgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred = xgb_clf.predict(X_test)\nxgb_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]\n\ndt_clf = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_train_eval(y_test, lr_clf, lr_pred, lr_pred_proba)\nget_model_train_eval(y_test, rf_clf, rf_pred, rf_pred_proba)\nget_model_train_eval(y_test, xgb_clf, xgb_pred, xgb_pred_proba)\nget_model_train_eval(y_test, dt_clf, dt_pred, dt_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 각 feature마다 target값에 따른 분포 확인하여 이상치 제거"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_drop_outlier = df_copy.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='0', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_0 = df_drop_outlier[df_drop_outlier['0'] > 0.25].index\ndf_drop_outlier.drop(drop_index_0, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='1', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_1 = df_drop_outlier[df_drop_outlier['1'] < -0.4].index\ndf_drop_outlier.drop(drop_index_1, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='2', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_2 = df_drop_outlier[(df_drop_outlier['2'] > 0.6) | (df_drop_outlier['2'] < -0.2)].index\ndf_drop_outlier.drop(drop_index_2, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='3', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_3 = df_drop_outlier[df_drop_outlier['3'] > 0.00].index\ndf_drop_outlier.drop(drop_index_3, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='4', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_4 = df_drop_outlier[df_drop_outlier['3'] > 0.00].index\ndf_drop_outlier.drop(drop_index_4, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='5', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_5 = df_drop_outlier[df_drop_outlier['5'] > 0.5].index\ndf_drop_outlier.drop(drop_index_5, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='6', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_6 = df_drop_outlier[df_drop_outlier['6'] > 0.2].index\ndf_drop_outlier.drop(drop_index_6, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"8\", y='7', data=df_drop_outlier, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index_7 = df_drop_outlier[df_drop_outlier['7'] > 0.5].index\ndf_drop_outlier.drop(drop_index_7, axis=0, inplace=True)\ndf_drop_outlier.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features = df_drop_outlier.iloc[:, :-1]\ny_target = df_drop_outlier.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n\nprint((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\nrf_clf = RandomForestClassifier(n_estimators=1000, max_depth=6, random_state=0)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\nxgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, min_child_weight=1.0, random_state=0)\nxgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred = xgb_clf.predict(X_test)\nxgb_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]\n\ndt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state=0)\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_train_eval(y_test, lr_clf, lr_pred, lr_pred_proba)\nget_model_train_eval(y_test, rf_clf, rf_pred, rf_pred_proba)\nget_model_train_eval(y_test, xgb_clf, xgb_pred, xgb_pred_proba)\nget_model_train_eval(y_test, dt_clf, dt_pred, dt_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBClassifier GridSearchCV 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {'learning_rate':[0.005,0.01,0.05], \n          'max_depth':[6,7,8],\n          'min_child_weight':[1.0,1.2],\n          'n_estimators':[500,800,1000],\n          'colsample_bytree':[0.5,0.8]}\n\ncv = KFold(n_splits=5, random_state=1)\ngrid_model = XGBClassifier(random_state=0)\nclf_cv = GridSearchCV(grid_model, params, cv=cv, n_jobs=4, verbose=1)\nclf_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('final params', clf_cv.best_params_)   # 최적의 파라미터 값 출력\nprint('best score', clf_cv.best_score_)      # 최고의 점수","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf_gs = XGBClassifier(**clf_cv.best_params_, random_state=0)\nxgb_clf_gs.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred_gs = xgb_clf_gs.predict(X_test)\nxgb_pred_gs_proba = xgb_clf_gs.predict_proba(X_test)[:, 1]\n\nget_model_train_eval(y_test, xgb_clf_gs, xgb_pred_gs, xgb_pred_gs_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier GridSearchCV 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'max_depth':[6,7,8],\n         'n_estimators':[500, 700, 900],\n         'min_samples_leaf':[3,5,7,10],\n         'min_samples_split':[2,3,5,10]}\n\ncv = KFold(n_splits=5, random_state=1)\ngrid_model_rf = RandomForestClassifier(random_state=0)\nclf_cv_rf = GridSearchCV(grid_model_rf, params, cv=cv, n_jobs=4, verbose=1)\nclf_cv_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('final params', clf_cv_rf.best_params_)   # 최적의 파라미터 값 출력\nprint('best score', clf_cv_rf.best_score_)      # 최고의 점수","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf_gs = RandomForestClassifier(**clf_cv_rf.best_params_)\nrf_clf_gs.fit(X_train, y_train)\nrf_pred_gs = rf_clf_gs.predict(X_test)\nrf_pred_gs_proba = rf_clf_gs.predict_proba(X_test)[:, 1]\n\nget_model_train_eval(y_test, rf_clf_gs, rf_pred_gs, rf_pred_gs_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_rf2 = rf_clf_gs.predict(test_df)\nensemble_pred = 0.6 * result_xgb2 + 0.4 * result_rf2 # xgb, rf2\nsubmission['Label'] = np.clip(ensemble_pred, 0 , max(ensemble_pred))\nsubmission['Label'] = submission['Label'].astype(int)\nsubmission.to_csv('diabetes_result6.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBClassifier GridSearchCV 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"params ={'learning_rate':[0.01, 0.05, 0.1],\n         'silent':[True],\n         'max_depth':[7,8,9],\n         'min_child_weight':[1.2,1.5,1.8],\n         'colsample_bytree':[0.5,0.8],\n         'colsample_bylevel':[0.9],\n         'n_estimators':[500, 700]}\n\ncv = KFold(n_splits=5, random_state=1)\ngrid_model2 = XGBClassifier(random_state=0)\nclf_cv2 = GridSearchCV(grid_model2, params, cv=cv, n_jobs=4, verbose=1)\nclf_cv2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('final params', clf_cv2.best_params_)   # 최적의 파라미터 값 출력\nprint('best score', clf_cv2.best_score_)      # 최고의 점수","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf_gs2 = XGBClassifier(**clf_cv2.best_params_)\nxgb_clf_gs2.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_train, y_train), (X_test, y_test)])\nxgb_pred_gs2 = xgb_clf_gs2.predict(X_test)\nxgb_pred_gs2_proba = xgb_clf_gs2.predict_proba(X_test)[:, 1]\n\nget_model_train_eval(y_test, xgb_clf_gs2, xgb_pred_gs2, xgb_pred_gs2_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_xgb2 = xgb_clf_gs2.predict(test_df)\nensemble_pred = 0.6 * result_xgb2 + 0.4 * result # xgb, xgb2\nsubmission['Label'] = np.clip(ensemble_pred, 0 , max(ensemble_pred))\nsubmission['Label'] = submission['Label'].astype(int)\nsubmission.to_csv('diabetes_result5.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb with grid_search\nresult = xgb_clf_gs.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_pred = 0.6 * result + 0.4 * result_rf # xgb, rf\nsubmission['Label'] = np.clip(ensemble_pred, 0 , max(ensemble_pred))\nsubmission['Label'] = submission['Label'].astype(int)\nsubmission.to_csv('diabetes_result4.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}